{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"C:/Users/Daniel/Documents/Masterarbeit/Relation_Selection_Test/article.csv\", sep=';', encoding=\"utf-8\")\n",
    "init_nodes = pd.read_csv(\"C:/Users/Daniel/Documents/Masterarbeit/Relation_Selection_Test/initial_nodes.csv\", sep=';',encoding=\"utf-8\")\n",
    "validation_data = pd.read_csv(\"C:/Users/Daniel/Documents/Masterarbeit/Relation_Selection_Test/entity_validation.csv\", sep=';',encoding=\"utf-8\")\n",
    "relation_validation_data = pd.read_csv(\"C:/Users/Daniel/Documents/Masterarbeit/Relation_Selection_Test/relation_validation.csv\", sep=';',encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "# load the functionalities\n",
    "from entitynetwork.enititynetwork_pipeline.process_article import *\n",
    "from entitynetwork.database_creation.fill_database import *\n",
    "from entitynetwork.relation_extraction.relation_extraction import *\n",
    "#from entitynetwork.validation_tool.relation_validation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Establishe DB Connection\n",
    "g = connect_database(password=\"Test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Tag selection\n",
    "\n",
    "selected_tag = [\"B-PERSON\", \"I-PERSON\", \"B-FAC\", \"I-FAC\", \"B-ORG\", \"I-ORG\",\n",
    "                \"B-LOC\", \"I-LOC\", \"B-WORK_OF_ART\", \"I-WORK_OF_ART\", \"B-LAW\", \"I-LAW\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Daniel\\AppData\\Local\\Temp\\tmpybpknemj\\config.json as plain json\n",
      "Did not use initialization regex that was passed: _context_layer._module.weight_hh.*\n",
      "Did not use initialization regex that was passed: _context_layer._module.weight_ih.*\n",
      "..\\aten\\src\\ATen\\native\\BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n",
      "C:\\Users\\Daniel\\Documents\\Masterarbeit\\CSS_Termpaper\\EntityNetwork\\entitynetwork\\relation_extraction\\relation_extraction.py:58: RuntimeWarning: Mean of empty slice\n",
      "  sentiment = np.nanmean(prod)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Coref Model: 47.97275471687317\n",
      "Use Coref Model: 65.75624465942383\n",
      "Replace Coref: 0.02593088150024414\n",
      "Relation Extraction: 0.5365650653839111\n",
      "__ArticleNode: 0.003989458084106445\n",
      "__Publuischer: 0.0\n",
      "__Author: 0.0\n",
      "__push article_node: 0.13862895965576172\n",
      "__push author_node: 0.021942615509033203\n",
      "__push publisher_node: 0.007977962493896484\n",
      "__commit_relationships: 0.34906768798828125\n",
      "__calculate_sub_graph_metrics: 0.8497006893157959\n",
      "Commit to db: 1.38130784034729\n",
      "115.95604586601257\n"
     ]
    }
   ],
   "source": [
    "# Start the validation and store the data\n",
    "i = 0\n",
    "#ent_val = validation_data[validation_data[\"text_id\"]==data[\"id\"][i]]\n",
    "#rel_val = relation_validation_data[relation_validation_data[\"text_id\"]==data[\"id\"][i]]\n",
    "#article = Article(data[\"text\"][i], data[\"author_extracted\"][i], data[\"date\"][i], data[\"source\"][i], data[\"id\"][i],\n",
    "#               download_model=False, selected_tags=selected_tag, n_shift=1,graph=g,initial_syn_sets=init_nodes)\n",
    "initial_entities_dict, initial_entities_df = create_synonym_dict(g)\n",
    "\n",
    "article = Article(data[\"text\"][i], data[\"author_extracted\"][i], data[\"date\"][i], data[\"source\"][i], data[\"id\"][i],\n",
    "               download_ner_model=False, selected_tags=selected_tag, n_shift=0, graph=g, commit=True,\n",
    "                  initial_entities = initial_entities_df,initial_syn_dict = initial_entities_dict,\n",
    "               validate=False, first_round=False, filter_by_constituency=False,path_coref_parser=\"C:/Users/Daniel/Documents/Masterarbeit/AllenNLP_Models/coref-spanbert-large-2020.02.27.tar.gz\")\n",
    "start = time.time()\n",
    "article.process_articles()\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#article.relation_dataset.df\n",
    "#article.relation_validation.metrics_df\n",
    "article.relation_validation.merged_data.to_csv(\"C:\\\\Users\\\\Daniel\\\\Documents\\\\Masterarbeit\\\\test_relations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "article.entity_validation.merged_data.to_csv(\"C:\\\\Users\\\\Daniel\\\\Documents\\\\Masterarbeit\\\\test_entities.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(article.coref_dict)\n",
    "article.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "article.relation_validation.merged_data[article.relation_validation.merged_data[\"entity_1_y\"] != \"na\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from entitynetwork.enititynetwork_pipeline.synonymset_iterable import *\n",
    "\n",
    "test_synset = Synset()\n",
    "test_synset.create_initial_synsets(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['sturgeon',\n",
       "  \"swa scotland's\",\n",
       "  'scotland',\n",
       "  \"nicola sturgeon scotland's\",\n",
       "  'nicola sturgeon',\n",
       "  \"sturgeon scotland''s\",\n",
       "  \"scotland's\",\n",
       "  'swa scotland'],\n",
       " ['office for national statistics', 'ons'],\n",
       " ['bma scotland',\n",
       "  'british medical association ',\n",
       "  'british medical association scotland',\n",
       "  'british medical association',\n",
       "  \"bma's\",\n",
       "  'bma',\n",
       "  \"scotland's\",\n",
       "  'british (p) medical association'],\n",
       " ['nhs information centre',\n",
       "  'nhs the nhs information centre',\n",
       "  'national health service',\n",
       "  'government eu',\n",
       "  'nhs nhs',\n",
       "  \"england's\",\n",
       "  'nhs',\n",
       "  'eu eu',\n",
       "  'health',\n",
       "  'nhs england',\n",
       "  \"government's nhs\",\n",
       "  'eu',\n",
       "  'england'],\n",
       " [\"ec's\", 'european commission', 'europe', 'ec', 'eu'],\n",
       " ['salmond',\n",
       "  'bma scotland holyrood',\n",
       "  'parliament',\n",
       "  \"salmond's\",\n",
       "  'salmond holyrood',\n",
       "  'holyrood scottish parliamentary',\n",
       "  'holyrood',\n",
       "  'holyroods health and sport committee',\n",
       "  'health and sport committee',\n",
       "  \"holyrood's\",\n",
       "  \"holyrood's health and sport committee\",\n",
       "  \"scottish parliament's\",\n",
       "  \"health and sport committee's\",\n",
       "  'scottish parliamentary',\n",
       "  'scottish parliament',\n",
       "  'salmond health'],\n",
       " ['alan mcdevitt'],\n",
       " [\"bma's scottish general practitioners' committee\"],\n",
       " ['snp hq',\n",
       "  \"snp'\",\n",
       "  'scottish national party',\n",
       "  \"snp's\",\n",
       "  'snp',\n",
       "  'nat',\n",
       "  'nats',\n",
       "  \"nats'\",\n",
       "  \"scottish national party's\"],\n",
       " [\"snp's holyrood\",\n",
       "  'parliament',\n",
       "  'westminster',\n",
       "  'holyrood snp',\n",
       "  'snp',\n",
       "  'snp the scottish parliament',\n",
       "  'holyrood',\n",
       "  'snp holyrood',\n",
       "  'holyroods health and sport committee',\n",
       "  'health and sport committee',\n",
       "  \"holyrood's\",\n",
       "  \"holyrood's health and sport committee\",\n",
       "  \"scottish parliament's\",\n",
       "  \"health and sport committee's\",\n",
       "  'scottish parliamentary',\n",
       "  'davidson holyrood',\n",
       "  'holyrood westminster',\n",
       "  'scottish parliament',\n",
       "  'westminster holyrood'],\n",
       " ['scottish labour',\n",
       "  'labour party',\n",
       "  'labour',\n",
       "  'scottish labour party',\n",
       "  'labour scottlan',\n",
       "  \"labour's\"],\n",
       " ['labour iain gray', 'iain gray', \"labour iain gray's\", 'gray'],\n",
       " ['salmond',\n",
       "  \"alex salmond's snp\",\n",
       "  \"salmond's\",\n",
       "  \"alex salmond's\",\n",
       "  \"salmond's snp\",\n",
       "  'snp alex salmond',\n",
       "  \"snp scotland's\",\n",
       "  'first minister',\n",
       "  'alex salmond',\n",
       "  \"alex salmond first minister's questions\",\n",
       "  'scotland',\n",
       "  'alex salmond scotland',\n",
       "  'snp scotland',\n",
       "  'alex'],\n",
       " ['snp bill', 'bill'],\n",
       " ['tories', 'tory party', 'conservatives', 'tory'],\n",
       " ['tavish scott'],\n",
       " ['gray annabel goldie', 'goldie', 'annabel goldie'],\n",
       " [\"uk government's\",\n",
       "  'westminster',\n",
       "  \"whitehall's\",\n",
       "  'britsh government',\n",
       "  'whitehall',\n",
       "  'westminster government',\n",
       "  'uk government',\n",
       "  'coalition government',\n",
       "  'government at westminster',\n",
       "  'government',\n",
       "  \"uk's\",\n",
       "  \"coalition's\",\n",
       "  \"westminster's\",\n",
       "  'uk',\n",
       "  'tory - libdem coalition',\n",
       "  'coalition'],\n",
       " ['gove', 'michael gove'],\n",
       " ['msps', 'salmond msps'],\n",
       " [\"scotland's\", 'labour scotland', 'scotland', \"labour scotland's\"],\n",
       " ['nhs britain', \"britain's\", 'britain'],\n",
       " ['treasury scotland'],\n",
       " ['uk uk',\n",
       "  'alcohol',\n",
       "  'aha',\n",
       "  'uk alcohol health alliance',\n",
       "  'alcohol health alliance'],\n",
       " ['swa)',\n",
       "  'scottish whisky association',\n",
       "  'scotch',\n",
       "  'swa',\n",
       "  \"swa's\",\n",
       "  'scotch whisky association'],\n",
       " ['earth'],\n",
       " ['may', 'theresa may'],\n",
       " ['westminster', 'european court', 'westminster the european court'],\n",
       " ['michael moore'],\n",
       " ['licensing act', 'licensing (scotland) act 2005'],\n",
       " ['department of health',\n",
       "  'alcohol concern the department of health',\n",
       "  'alcohol concern',\n",
       "  'alcohol concern nhs'],\n",
       " ['uk don shenker', 'don shenker', 'shenker'],\n",
       " ['doh',\n",
       "  'department of health',\n",
       "  'government',\n",
       "  'government the department of health',\n",
       "  'department for health',\n",
       "  'uk government'],\n",
       " ['gilmore', 'ian gilmore', 'ian'],\n",
       " ['lamont bbc scotland', 'bbc'],\n",
       " ['asi', 'adam smith institute'],\n",
       " ['milton', 'anne milton', \"westminster's anne milton\"],\n",
       " ['tim straughan'],\n",
       " ['a & e'],\n",
       " ['north - west', 'north - east'],\n",
       " ['london'],\n",
       " ['chris sorek'],\n",
       " ['drinka - ware'],\n",
       " ['portman group'],\n",
       " ['david poley'],\n",
       " ['diane abbott', 'abbott', 'abbot', 'diane abbot'],\n",
       " ['diageo smirnoff', 'diageo'],\n",
       " ['simon litherland'],\n",
       " ['guinness'],\n",
       " ['sturgeon',\n",
       "  'nicola sturgeon bill',\n",
       "  'holyrood nicola sturgeon',\n",
       "  'nicola sturgeon eu',\n",
       "  'nicola sturgeon',\n",
       "  'sturgeon bill',\n",
       "  'nicola sturgeon holyrood'],\n",
       " ['snp snp government',\n",
       "  'scottish government',\n",
       "  'snp',\n",
       "  \"scottish government's\",\n",
       "  'nationalist government',\n",
       "  'snp government'],\n",
       " ['sturgeon',\n",
       "  'scotland',\n",
       "  \"sturgeon's\",\n",
       "  \"nicola sturgeon scotland's\",\n",
       "  'nicola sturgeon',\n",
       "  'sturgeon scotland',\n",
       "  \"sturgeon scotland's\",\n",
       "  \"scotland's\",\n",
       "  'nicola sturgeon scotland'],\n",
       " ['liberal democrats', 'libdems', 'lib dems'],\n",
       " ['willie rennie', \"holyrood's willie rennie\", 'rennie'],\n",
       " ['sturgeon scotland', \"sturgeon scotland's\", 'scotland'],\n",
       " ['scottish licensed trade association',\n",
       "  'slta',\n",
       "  'scottish whisky association'],\n",
       " ['paul waterson'],\n",
       " ['balance'],\n",
       " ['david annand'],\n",
       " ['snp winnie ewing'],\n",
       " ['snp sean connery'],\n",
       " [\"snp's msps\",\n",
       "  'eu the snp government',\n",
       "  'snp government msps',\n",
       "  'snp msps',\n",
       "  'snp'],\n",
       " ['a minimum pricing bill',\n",
       "  'alcohol minimum pricing scotland bill',\n",
       "  'minimum pricing bill',\n",
       "  'alcohol minimum pricing bill',\n",
       "  'an alcohol minimum pricing bill'],\n",
       " ['ed miliband england', 'miliband', 'ed miliband'],\n",
       " ['cathy jamieson', 'border', 'cathy jamieson border', 'jamieson'],\n",
       " ['snpuk'],\n",
       " ['police',\n",
       "  'strathclyde police',\n",
       "  'police officer',\n",
       "  'strathclyde',\n",
       "  'strathclyde police authority jack mcconnell s',\n",
       "  'police authority',\n",
       "  'strathclyde police authority'],\n",
       " ['north lanarkshire labour'],\n",
       " ['michael ross'],\n",
       " ['strathclyde', 'glasgow', 'glasgow strathclyde'],\n",
       " ['stephen house'],\n",
       " ['jamieson kilmarnock'],\n",
       " ['loudoun'],\n",
       " ['labour uk', 'uk labour'],\n",
       " ['cllr ross'],\n",
       " ['glasgow south', 'glasgow'],\n",
       " ['stephen curran'],\n",
       " ['labour uk'],\n",
       " [\"registrar general's annual report\"],\n",
       " ['david cameron', 'cameron', 'prime minister'],\n",
       " ['cameron @ the - sun. co. uk', '- sun. co. uk the sun'],\n",
       " [\"panorama's dying for a drink\", 'drink'],\n",
       " ['victoria',\n",
       "  'centre for addictions research, university of victoria',\n",
       "  'university of victoria',\n",
       "  \"university of victoria's centre for addictions research\",\n",
       "  'centre for addictions research'],\n",
       " ['sky living'],\n",
       " ['msp kenneth gibson', 'gibson', 'msp', 'kenneth gibson'],\n",
       " ['gibson fasd'],\n",
       " ['children in scotland'],\n",
       " ['salmondwestminster'],\n",
       " ['big tobacco'],\n",
       " ['sab miller', 'sab miller eu'],\n",
       " ['france', 'uk france'],\n",
       " ['ireland'],\n",
       " ['kristin wolfe'],\n",
       " [\"tennent's\"],\n",
       " ['labour lib dems'],\n",
       " ['wales'],\n",
       " ['price'],\n",
       " ['nhs health scotland', 'nhs scotland', \"scotland's health\"],\n",
       " ['bma scotland brian keighley', 'brian keighley'],\n",
       " ['murdo fraser', 'fraser', \"murdo fraser davidson's\"],\n",
       " [\"labour's jackie baillie\"],\n",
       " [\"andrewnicoll @ the - sun. co. uk city's\"],\n",
       " ['ukscotland'],\n",
       " ['wine & spirit trade association',\n",
       "  'wine and spirits association',\n",
       "  'wine and spirit trade association',\n",
       "  'wsta)',\n",
       "  'wines and spirits trade association',\n",
       "  'wsta',\n",
       "  'wine and spirits trade association'],\n",
       " ['jeremy beadles'],\n",
       " ['centre for economic and business research',\n",
       "  'centre for economic business research',\n",
       "  'cebr)',\n",
       "  'cebr'],\n",
       " ['cebr benjamin williamson', 'cebr)', 'cebr'],\n",
       " ['cebr benjamin williamson'],\n",
       " ['tesco asda', 'asda'],\n",
       " ['tesco', 'tesco england'],\n",
       " ['threshers'],\n",
       " ['oddbins'],\n",
       " ['laithwaites'],\n",
       " ['amazon'],\n",
       " ['sbpa', 'scottish beer and pub association'],\n",
       " ['patrick browne'],\n",
       " ['hm revenue and customs'],\n",
       " ['herald'],\n",
       " ['virgin wines'],\n",
       " ['majestic'],\n",
       " ['marks & spencer'],\n",
       " ['campbell evans'],\n",
       " ['george samra', 'samra'],\n",
       " ['costcutter'],\n",
       " ['gretna'],\n",
       " ['sgf',\n",
       "  'wsta) the scottish grocers federation',\n",
       "  'scottish grocers federation'],\n",
       " ['spar'],\n",
       " ['scotmid'],\n",
       " ['keystore'],\n",
       " ['drummond', 'john drummond', 'sgf john drummond'],\n",
       " ['borders', 'border'],\n",
       " ['alcohol focus scotland scottish health action on alcohol problems',\n",
       "  'afs',\n",
       "  'alcohol focus scotland',\n",
       "  'alcohol (scotland) act'],\n",
       " ['samra'],\n",
       " ['budweiser'],\n",
       " ['carlisle'],\n",
       " ['karen callaghan'],\n",
       " ['gary nicholson'],\n",
       " ['bargain booze'],\n",
       " ['stanwix'],\n",
       " ['andrew thomas'],\n",
       " ['frosty jack'],\n",
       " ['zepplin'],\n",
       " ['jackie scott'],\n",
       " ['royal liverpool university', 'liverpool'],\n",
       " ['bradford', 'bradford university'],\n",
       " ['british retail consortium', 'british retail consortium food', 'brc'],\n",
       " ['andrew opie'],\n",
       " ['alcohol research uk'],\n",
       " ['davidson', \"davidson's\", 'ruth davidson'],\n",
       " ['yorkshire'],\n",
       " ['robin davidson'],\n",
       " ['royal liverpool hospital'],\n",
       " ['royal college of physicians ', 'royal college of physicians', 'rcp'],\n",
       " ['italy', 'spain italy'],\n",
       " ['spain'],\n",
       " ['lancet'],\n",
       " ['curtin university', 'curtin university australia'],\n",
       " ['sheffield university',\n",
       "  'university of sheffield',\n",
       "  'uk the university of sheffield'],\n",
       " ['martin hagger', 'hagger'],\n",
       " ['ias',\n",
       "  'institute of alcohol studies',\n",
       "  'national institute of alcohol studies'],\n",
       " ['nicola sturgeon labour',\n",
       "  'nicola sturgeon',\n",
       "  'sturgeon',\n",
       "  \"sturgeon labour's\"],\n",
       " ['snp sturgeon'],\n",
       " ['pamela nash', 'nash', 'labour pamela nash'],\n",
       " ['european union'],\n",
       " ['law society of scotland', \"law society's\", 'law society'],\n",
       " ['legal'],\n",
       " ['commons',\n",
       "  'british parliament',\n",
       "  'house of commons',\n",
       "  'house of commons scotland'],\n",
       " ['greene king', 'greene king uk'],\n",
       " ['damian green', 'green'],\n",
       " ['rooney anand', 'greene king', 'greene king rooney anand'],\n",
       " ['tim stockwell', 'snp tim stockwell'],\n",
       " ['centre for addictions research british columbia', 'british columbia'],\n",
       " ['canadian', 'canada'],\n",
       " ['snpthe scottish parliament'],\n",
       " ['snpholyrood'],\n",
       " ['politics show'],\n",
       " ['msps', 'labour msps'],\n",
       " ['lamont health'],\n",
       " ['ken macintosh', 'macintosh', 'eastwood msp ken macintosh'],\n",
       " ['tom harris', 'harris'],\n",
       " ['macintosh scotland', 'scotland'],\n",
       " ['calman commission'],\n",
       " ['lamont labour'],\n",
       " ['united kingdom'],\n",
       " ['carlaw',\n",
       "  'jackson carlaw',\n",
       "  'jackson carlaw margaret mitchell',\n",
       "  'margaret mitchell'],\n",
       " ['scottish health action on alcohol problems', 'shaap'],\n",
       " ['evelyn gillan'],\n",
       " ['swa) coca - cola', 'swa'],\n",
       " ['eu james mclean', 'mclean', 'james mclean'],\n",
       " ['sturgeon sheffield university', 'sheffield university'],\n",
       " ['chief medical officer', 'cmo'],\n",
       " ['liam donaldson'],\n",
       " ['rcn', 'royal college of nursing', 'royal college of nursing scotland'],\n",
       " ['theresa fyffe'],\n",
       " ['gavin hewitt'],\n",
       " ['controversial coalition'],\n",
       " ['responsibility deals'],\n",
       " ['stephen dorrell', 'dorrell'],\n",
       " ['lords'],\n",
       " ['health and social care bill'],\n",
       " ['faculty of public health',\n",
       "  'public health',\n",
       "  'uk faculty of public health',\n",
       "  'fph'],\n",
       " ['government bill', 'governmentbill', 'bill'],\n",
       " ['labour richard simpson',\n",
       "  \"labour's richard simpson\",\n",
       "  'labourrichard simpson',\n",
       "  'richard simpson',\n",
       "  'simpson'],\n",
       " ['alison mcinnes', 'mcinnes', 'alison mcinnes scotland'],\n",
       " ['medics'],\n",
       " [\"house of commons' health committee\",\n",
       "  'health committee',\n",
       "  'commons health select committee',\n",
       "  'select committee',\n",
       "  'commons health committee',\n",
       "  'health select committee'],\n",
       " ['commons nhs'],\n",
       " ['lansley',\n",
       "  \"andrew lansley's public health responsibility deal\",\n",
       "  'andrew lansley health',\n",
       "  'andrew lansley'],\n",
       " ['jamie oliver'],\n",
       " ['ed jessop'],\n",
       " ['record'],\n",
       " ['ruinous'],\n",
       " ['scotland', 'government scotland'],\n",
       " ['scottish crime and justice survey'],\n",
       " ['new'],\n",
       " ['swa) the european court of justice', 'european court of justice'],\n",
       " ['sgf)', 'wsta sgf'],\n",
       " ['government parliament'],\n",
       " ['community pharmacy scotland'],\n",
       " ['health', 'conservative health'],\n",
       " [\"society's competition law and licensing law\"],\n",
       " ['government mclean'],\n",
       " ['lansley labour'],\n",
       " ['local alcohol profiles for england'],\n",
       " ['eric appleby'],\n",
       " ['neil', 'alex neil'],\n",
       " ['neilsen', 'neilson'],\n",
       " ['john holmes'],\n",
       " ['bob doris', 'doris', 'snp msp bob doris'],\n",
       " [\"davidson scotland's\"],\n",
       " ['times'],\n",
       " ['johann lamont', 'lamont', 'john lamont'],\n",
       " ['david mcletchie'],\n",
       " ['chloe smith', 'smith'],\n",
       " ['liz smith'],\n",
       " ['economy, energy and tourism committee'],\n",
       " ['glasgow caledonian university', 'glasgow caledonian'],\n",
       " ['laura williamson', 'williamson'],\n",
       " ['wellcome trust'],\n",
       " ['institute for applied health research'],\n",
       " ['alcohol and alcoholism'],\n",
       " ['national institute for health and clinical excellence'],\n",
       " ['government the daily telegraph', 'daily telegraph'],\n",
       " ['katherine brown'],\n",
       " ['gavin partington'],\n",
       " ['sarah wollaston', 'wollaston', 'sarah wollaston government'],\n",
       " ['john carnochan'],\n",
       " ['scottish violence reduction unit'],\n",
       " ['kenny macaskill'],\n",
       " ['charlene wilson'],\n",
       " ['gail russell'],\n",
       " ['blantyre'],\n",
       " ['cameron scotland'],\n",
       " ['brake'],\n",
       " ['direct line'],\n",
       " ['chris auld']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_synset.synonym_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from allennlp.predictors.predictor import Predictor\n",
    "import allennlp_models.structured_prediction\n",
    "#open_ie_predictor = Predictor.from_path(\"C:/Users/Daniel/Documents/Masterarbeit/AllenNLP_Models/openie-model.2020.03.26.tar.gz\")\n",
    "#constituency_predictor = Predictor.from_path(\"C:/Users/Daniel/Documents/Masterarbeit/AllenNLP_Models/elmo-constituency-parser-2020.02.10.tar.gz\")\n",
    "#dependency_predictor = Predictor.from_path(\"C:/Users/Daniel/Documents/Masterarbeit/AllenNLP_Models/biaffine-dependency-parser-ptb-2020.04.06.tar.gz\")\n",
    "#semantic_roll_predictor = Predictor.from_path(\"C:/Users/Daniel/Documents/Masterarbeit/AllenNLP_Models/bert-base-srl-2020.03.24.tar.gz\")\n",
    "#coref_predictor = Predictor.from_path(\"C:/Users/Daniel/Documents/Masterarbeit/AllenNLP_Models/coref-spanbert-large-2020.02.27.tar.gz\")\n",
    "sentiment_predictor = Predictor.from_path(\"C:/Users/Daniel/Documents/Masterarbeit/AllenNLP_Models/sst-roberta-large-2020.06.08.tar.gz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prediction = open_ie_predictor.predict(sentence=\"DRINKERS will pay a minimum price for alcohol under plans instigated by David Cameron to tackle a growing health crisis, The Daily Telegraph can disclose. \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_prediction_df(prediction):\n",
    "    len_prediction = len(prediction.get(\"verbs\"))\n",
    "    prediction_df = pd.DataFrame({\"words\": prediction.get(\"words\")})\n",
    "    for i in range(len_prediction):\n",
    "        col = \"tags_\" + str(i)\n",
    "        prediction_df[col] = prediction.get(\"verbs\")[i].get(\"tags\")\n",
    "    return prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "arg_df = get_prediction_df(prediction)\n",
    "arg_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "searchterm = \"minimum price for alcohol\"\n",
    "tokens = searchterm.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def find_sub_list(sl,l, arg_df):\n",
    "    #results=[]\n",
    "    sll=len(sl)\n",
    "    for ind in (i for i,e in enumerate(l) if e==sl[0]):\n",
    "        if l[ind:ind+sll]==sl:\n",
    "            arg_df[\"entity\"][ind:ind+sll] = 1\n",
    "            #results.append((ind,ind+sll-1))\n",
    "\n",
    "    return arg_df #results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "found_index = find_sub_list(tokens, prediction.get(\"words\"))\n",
    "found_index[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "arg_df[\"match\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "arg_df[\"match\"][found_index[0][0]:found_index[0][1]+1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "arg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "article.entity_dataset.df[\"synonyms\"][article.occurrence_matrix[:,0] == 1].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_prediction_df(article):\n",
    "    complete_df = pd.DataFrame()\n",
    "    for k in range(len(article.sentences)):\n",
    "        prediction = open_ie_predictor.predict(sentence=article.sentences[k])\n",
    "        len_prediction = len(prediction.get(\"verbs\"))\n",
    "        prediction_df = pd.DataFrame({\"words\": prediction.get(\"words\")})\n",
    "        sentence_tokenized = [word.lower() for word in prediction.get(\"words\")]\n",
    "        prediction_df[\"entity\"] = 0\n",
    "        for i in range(len_prediction):\n",
    "            col = \"tags_\" + str(i)\n",
    "            prediction_df[col] = prediction.get(\"verbs\")[i].get(\"tags\")\n",
    "        search_terms = article.entity_dataset.df[\"synonyms\"][article.occurrence_matrix[:,k] == 1]\n",
    "        index_terms= search_terms.index\n",
    "        for i in range(len(index_terms)):\n",
    "            synonym_array = article.entity_dataset.get_synonym_array(index_terms[i])\n",
    "            for j in range(len(synonym_array)):\n",
    "                tokens = synonym_array[j].split()\n",
    "                prediction_df = find_sub_list(tokens,sentence_tokenized, prediction_df)\n",
    "        complete_df = pd.concat([complete_df,prediction_df], axis=0, ignore_index=True)\n",
    "    return complete_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "complete_df = get_prediction_df(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "complete_df.to_csv(\"C:\\\\Users\\\\Daniel\\\\Documents\\\\Masterarbeit\\\\relation_predition.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#open_ie = open_ie_predictor.predict(article.sentences[0])\n",
    "constituency = constituency_predictor.predict(article.sentences[0])\n",
    "#dependency = dependency_predictor.predict(\"DRINKERS will pay a minimum price for alcohol under plans instigated by David Cameron to tackle a growing health crisis, The Daily Telegraph can disclose.\")\n",
    "#semantic_roll = semantic_roll_predictor.predict(sentence= article.sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#print(constituency.get(\"hierplane_tree\").get(\"root\").keys())\n",
    "d = constituency.get(\"hierplane_tree\").get(\"root\")\n",
    "#dependency_df = pd.DataFrame({\"words\": dependency.get(\"words\")})\n",
    "#dependency_df[\"pos\"] = dependency.get(\"pos\")\n",
    "#dependency_df[\"head\"] = dependency.get(\"predicted_heads\")\n",
    "#dependency_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_prediction_df(prediction):\n",
    "    len_prediction = len(prediction.get(\"verbs\"))\n",
    "    prediction_df = pd.DataFrame({\"words\": prediction.get(\"words\")})\n",
    "    for i in range(len_prediction):\n",
    "        col = \"tags_\" + str(i)\n",
    "        prediction_df[col] = prediction.get(\"verbs\")[i].get(\"tags\")\n",
    "    return prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# This class represents a directed graph\n",
    "# using adjacency list representation\n",
    "class Graph:\n",
    "\n",
    "    def __init__(self, vertices):\n",
    "        # No. of vertices\n",
    "        self.V = vertices\n",
    "\n",
    "        # default dictionary to store graph\n",
    "        self.graph = defaultdict(list)\n",
    "\n",
    "    # function to add an edge to graph\n",
    "    def addEdge(self, u, v):\n",
    "        self.graph[u].append(v)\n",
    "\n",
    "    '''A recursive function to print all paths from 'u' to 'd'.\n",
    "    visited[] keeps track of vertices in current path.\n",
    "    path[] stores actual vertices and path_index is current\n",
    "    index in path[]'''\n",
    "    def printAllPathsUtil(self, u, d, visited, path):\n",
    "\n",
    "        # Mark the current node as visited and store in path\n",
    "        visited[u]= True\n",
    "        path.append(u)\n",
    "\n",
    "        # If current vertex is same as destination, then print\n",
    "        # current path[]\n",
    "        if u == d:\n",
    "            print(path)\n",
    "        else:\n",
    "            # If current vertex is not destination\n",
    "            # Recur for all the vertices adjacent to this vertex\n",
    "            for i in self.graph[u]:\n",
    "                if visited[i]== False:\n",
    "                    self.printAllPathsUtil(i, d, visited, path)\n",
    "\n",
    "        # Remove current vertex from path[] and mark it as unvisited\n",
    "        path.pop()\n",
    "        visited[u]= False\n",
    "\n",
    "\n",
    "    # Prints all paths from 's' to 'd'\n",
    "    def printAllPaths(self, s, d):\n",
    "\n",
    "        # Mark all the vertices as not visited\n",
    "        visited =[False]*(self.V)\n",
    "\n",
    "        # Create an array to store paths\n",
    "        path = []\n",
    "\n",
    "        # Call the recursive helper function to print all paths\n",
    "        self.printAllPathsUtil(s, d, visited, path)\n",
    "\n",
    "\n",
    "\n",
    "# Create a graph given in the above diagram\n",
    "g = Graph((len(dependency_df)+1))\n",
    "for i in range(len(dependency_df)):\n",
    "    g.addEdge((i+1),dependency_df[\"head\"][i])\n",
    "\n",
    "s = 20 ; d = 0\n",
    "print (\"Following are all different paths from % d to % d :\" %(s, d))\n",
    "g.printAllPaths(s, d)\n",
    "# This code is contributed by Neelam Yadav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dependency_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "g.printAllPaths(25,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "re.findall(\"[\\{\\}]\" ,str(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def fun(dct, value, path=()):\n",
    "    for key, val in dct.items():\n",
    "        if val == value:\n",
    "            yield path + (key, )\n",
    "    for key, lst in dct.items():\n",
    "        if isinstance(lst, list):\n",
    "            for item in lst:\n",
    "                for pth in fun(item, value, path + (dct['id'], key, )):\n",
    "                    yield pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for item in fun(d, value='NP'):\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def get_paths(source):\n",
    "    paths = []\n",
    "    if isinstance(source, collections.MutableMapping):  # found a dict-like structure...\n",
    "        for k, v in source.items():  # iterate over it; Python 2.x: source.iteritems()\n",
    "            paths.append([k])  # add the current child path\n",
    "            paths += [[k] + x for x in get_paths(v)]  # get sub-paths, extend with the current\n",
    "    # else, check if a list-like structure, remove if you don't want list paths included\n",
    "    elif isinstance(source, collections.Sequence) and not isinstance(source, str):\n",
    "        #                          Python 2.x: use basestring instead of str ^\n",
    "        for i, v in enumerate(source):\n",
    "            paths.append([i])\n",
    "            paths += [[i] + x for x in get_paths(v)]  # get sub-paths, extend with the current\n",
    "    return paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json as JSON\n",
    "dj = JSON.dumps(d)\n",
    "dj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_constituency_path(dct, value, path=()):\n",
    "    #for key, val in dct.items():\n",
    "    if value.lower() in dct.get(\"word\").lower(): #val == value:\n",
    "        yield path# + (dct.get(\"attributes\"), )\n",
    "    for key, lst in dct.items():\n",
    "        if isinstance(lst, list):\n",
    "            for item in lst:\n",
    "                #print(item)\n",
    "                if isinstance(item,dict):\n",
    "                    for pth in get_constituency_path(item, value, path + (item.get(\"attributes\"), )):\n",
    "                        yield pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "path = ()\n",
    "for key, val in d.items():\n",
    "    print(key)\n",
    "    print(val)\n",
    "    path = path + (d.get(\"attributes\"), )\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "e_1 = ()\n",
    "for item in get_constituency_path(d,\"David Cameron\", path=(d.get(\"attributes\"), )):\n",
    "    e_1 = item\n",
    "e_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for item in get_constituency_path(d,\"minimum price for alcohol\", path=(d.get(\"attributes\"), )):\n",
    "    e_2 = item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def is_valid(path_1, path_2):\n",
    "    valid = 0\n",
    "    for idx, (i, j) in enumerate(zip(path_1, path_2)):\n",
    "        if i == j:\n",
    "            pass\n",
    "        else:\n",
    "            split_point = (i,j)\n",
    "            if [\"NP\"] in split_point:\n",
    "                if any(tag in split_point for tag in [[\"VP\"],[\"PP\"]]):\n",
    "                    valid = 1\n",
    "                    return valid\n",
    "                elif[\"NP\"] == split_point[0] or [\"NP\"] == split_point[1]:\n",
    "                    valid = 1\n",
    "                    return valid\n",
    "    return valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def filter_by_constituency(article):\n",
    "    article.relation_validation.merged_data[\"constituency_path\"] = \"\"\n",
    "    for k in range(len(article.sentences)):\n",
    "        entity_names = article.entity_dataset.df[\"name\"][article.occurrence_matrix[:,k] == 1]\n",
    "        search_terms = article.entity_dataset.df[\"synonyms\"][article.occurrence_matrix[:,k] == 1]\n",
    "        index_terms = search_terms.index\n",
    "        constituency = constituency_predictor.predict(article.sentences[k])\n",
    "        constituency = constituency.get(\"hierplane_tree\").get(\"root\")\n",
    "        path_dic = dict()\n",
    "        for i in range(len(index_terms)):\n",
    "            synonym_array = article.entity_dataset.get_synonym_array(index_terms[i])\n",
    "            lst = []\n",
    "            for j in range(len(synonym_array)):\n",
    "                path = ()\n",
    "                for item in get_constituency_path(constituency, synonym_array[j], (constituency.get(\"attributes\"), )):\n",
    "                    path = item\n",
    "                if len(path) > 0:\n",
    "                    lst.append(path)\n",
    "            path_dic[entity_names[index_terms[i]]] = lst\n",
    "        for i in range(len(index_terms)):\n",
    "            for i_values in path_dic.get(entity_names[index_terms[i]]):\n",
    "                for j in range(i + 1, len(index_terms) ):\n",
    "                    for j_values in path_dic.get(entity_names[index_terms[j]]):\n",
    "                        #print(entity_names[index_terms[i]] + \" - \" + entity_names[index_terms[j]])\n",
    "                        #print(get_split_point(i_values,j_values))\n",
    "                        article.relation_validation.merged_data[\n",
    "                            \"constituency_path\"][(article.relation_validation.merged_data[\n",
    "                                                              \"entity_1_x\"] == entity_names[index_terms[i]]) &\n",
    "                                     (article.relation_validation.merged_data[\"entity_2_x\"] ==\n",
    "                                      entity_names[index_terms[j]])] += str(is_valid(i_values,j_values))\n",
    "\n",
    "    return path_dic\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "article.relation_validation.merged_data[\"count\"][(article.relation_validation.merged_data[\"entity_1_x\"] == \"downing street\") & \\\n",
    "(article.relation_validation.merged_data[\"entity_2_x\"] == \"minimum price per unit\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "filter_by_constituency(article)\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "article.relation_validation.merged_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "split_point = get_split_point(e_1,e_2)\n",
    "print(split_point)\n",
    "any(item in split_point for item in [[\"NP\"],[\"PP\"],[\"VP\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "split_point = (['NP'],['NP'], 1)\n",
    "valid = 0\n",
    "if [\"NP\"] in split_point:\n",
    "    if any(tag in split_point for tag in [[\"VP\"],[\"PP\"]]):\n",
    "        valid = 1\n",
    "    elif[\"NP\"] == split_point[0] or [\"NP\"] == split_point[1]:\n",
    "        valid = 1\n",
    "\n",
    "\n",
    "[\"NP\"] == split_point[0] or [\"NP\"] == split_point[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "coref = coref_predictor.predict(data[\"text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "coref.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "coref.get('predicted_antecedents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "coref_cluster = coref.get(\"clusters\")\n",
    "coref_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "text_tokenized = coref.get('document')\n",
    "len(text_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def join_and_clean(word_list):\n",
    "    joint_string = ' '.join(word_list)\n",
    "    joint_string = re.sub(r'\\s+([?.!,\" \\'])', r'\\1', joint_string)\n",
    "    return joint_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for entities in coref_cluster:\n",
    "    replacement_list = text_tokenized[entities[0][0]:(entities[0][1]+1)]\n",
    "    replacement =  join_and_clean(replacement_list)\n",
    "    for synonyms in entities:\n",
    "        print(synonyms[0])\n",
    "        text_tokenized[synonyms[0]] = replacement\n",
    "        text_tokenized[(synonyms[0]+1):(synonyms[1]+1)] = [\"\"]*(synonyms[1]-synonyms[0])\n",
    "\n",
    "#coref_cluster[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "joint_text = join_and_clean(text_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "joint_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokenize.sent_tokenize(joint_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "text_dic = {\"the prime minister\": 1, \"the prime minister david cameron\": 1}\n",
    "search = \"the prime minister\"\n",
    "result = [[key, text_dic[key]] for key in text_dic if search in key.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(len(result))\n",
    "result_df = pd.DataFrame(result,columns=[\"key\",\"coref_nr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(set(result_df[\"coref_nr\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "lengths = [len(key) for key in result_df[\"key\"]]\n",
    "result_df[\"coref_nr\"][min(enumerate(lengths), key=itemgetter(1))[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_coref_set_nr(self,new_entity, coref_dict):\n",
    "    result = [[key, coref_dict[key]] for key in coref_dict if new_entity.name in key.lower()]\n",
    "    if len(result) == 0:\n",
    "        return \"Entity dose not exists\"\n",
    "    result_df = pd.DataFrame(result,columns=[\"key\",\"coref_nr\"])\n",
    "    if len(set(result_df[\"coref_nr\"])) == 1:\n",
    "        new_entity.coref_nr = result[0][1]\n",
    "    else:\n",
    "        new_entity.coref_nr = result_df[\"coref_nr\"][min(enumerate(lengths), key=itemgetter(1))[0]]\n",
    "    entity_index = self[self[\"coref_nr\"] == new_entity.coref_nr].index.value[0]\n",
    "    return entity_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "result_df[result_df[\"key\"] == \"the prime minister\"].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from Giveme5W1H.extractor.document import Document\n",
    "from Giveme5W1H.extractor.extractor import MasterExtractor\n",
    "\n",
    "extractor = MasterExtractor()\n",
    "doc = Document.from_text(data[\"text\"][0], data[\"date\"][0])\n",
    "# or: doc = Document(title, lead, text, date_publish)\n",
    "doc = extractor.parse(doc)\n",
    "\n",
    "top_who_answer = doc.get_top_answer('who').get_parts_as_text()\n",
    "print(top_who_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from py2neo import Graph\n",
    "\n",
    "from entitynetwork.enititynetwork_pipeline.synonym_dict import create_synonym_dict\n",
    "\n",
    "syn_dict = create_synonym_dict(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "value = syn_dict.get(\"swkja\")\n",
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_nodes = g.run('MATCH (n:Entity) RETURN n.name AS name, n.tag AS tag, n.synonyms AS synonyms, '\n",
    "                           'n.locked AS locked, ID(n) AS id').to_data_frame()\n",
    "len(init_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.00695"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.prod([article.sentiment_array, article.occurrence_matrix[1, :-1]], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.5859     nan     nan     nan  0.         nan     nan     nan     nan\n",
      "     nan     nan     nan     nan     nan     nan     nan     nan     nan\n",
      "     nan     nan     nan     nan     nan     nan     nan     nan     nan\n",
      "     nan     nan     nan     nan  0.1689     nan     nan     nan     nan\n",
      "     nan     nan     nan     nan     nan     nan     nan     nan     nan\n",
      "     nan     nan     nan     nan     nan     nan     nan     nan     nan\n",
      "     nan     nan     nan     nan     nan     nan]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.13899999999999998"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prod = np.prod([article.sentiment_array, article.occurrence_matrix[1, :-1]], axis=0)\n",
    "prod[article.occurrence_matrix[1, :-1] == 0] = np.nan\n",
    "print(prod)\n",
    "np.nanmean(prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>db_id_1</th>\n",
       "      <th>db_id_2</th>\n",
       "      <th>count</th>\n",
       "      <th>weight</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>filter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.585900</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.830508</td>\n",
       "      <td>0.226300</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.179800</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.932203</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.585900</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.474576</td>\n",
       "      <td>0.168900</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.474576</td>\n",
       "      <td>0.168900</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.830508</td>\n",
       "      <td>0.226300</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.135593</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.372881</td>\n",
       "      <td>-0.092133</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.355932</td>\n",
       "      <td>-0.802000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.474576</td>\n",
       "      <td>0.168900</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>14.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.355932</td>\n",
       "      <td>-0.802000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>16.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.288136</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    db_id_1  db_id_2  count    weight  sentiment  filter\n",
       "0       3.0      4.0    1.0  1.000000  -0.585900     0.0\n",
       "1       3.0      6.0    1.0  0.830508   0.226300     0.0\n",
       "2       3.0     10.0    2.0  1.000000  -0.179800     0.0\n",
       "3       4.0      6.0    1.0  0.932203   0.000000     0.0\n",
       "4       4.0     10.0    1.0  1.000000  -0.585900     0.0\n",
       "5       4.0     11.0    1.0  0.474576   0.168900     0.0\n",
       "6       4.0     12.0    1.0  0.474576   0.168900     0.0\n",
       "7       6.0     10.0    1.0  0.830508   0.226300     0.0\n",
       "8       6.0     19.0    1.0  0.135593   0.000000     0.0\n",
       "9      10.0     14.0    3.0  0.372881  -0.092133     0.0\n",
       "10     10.0     15.0    1.0  0.355932  -0.802000     0.0\n",
       "11     11.0     12.0    1.0  0.474576   0.168900     0.0\n",
       "12     14.0     15.0    1.0  0.355932  -0.802000     0.0\n",
       "13     16.0     17.0    2.0  0.288136   0.106000     0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article.relation_dataset.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load the necessary data\n",
    "import pandas as pd\n",
    "data = pd.read_csv(\"/home/ubuntu/ma_project/mup_valid_articles.csv\", encoding=\"utf-8\")\n",
    "validation_data = pd.read_csv(\"/home/ubuntu/ma_project/entity_dataframe_lowercase.csv\",encoding=\"utf-8\")\n",
    "relation_validation_data = pd.read_csv(\"/home/ubuntu/ma_project/relation_dataframe_lowercase.csv\",encoding=\"utf-8\")\n",
    "initial_nodes = pd.read_csv(\"/home/ubuntu/ma_project/MUP_nodes.csv\",sep = \";\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load the functionalities\n",
    "from entitynetwork.enititynetwork_pipeline.process_article import *\n",
    "from entitynetwork.database_creation.fill_database import *\n",
    "from entitynetwork.relation_extraction.relation_extraction import *\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "#from entitynetwork.validation_tool.relation_validation import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Establishe DB Connection\n",
    "global g\n",
    "g = connect_database(db_name = \"testimprovements\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "constituency_predictor_1 = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/elmo-constituency-parser-2020.02.10.tar.gz\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "initial_entities_dict, initial_entities_df = create_synonym_dict(g, None)\n",
    "global_start = time.time()\n",
    "for i in range(10, 15):\n",
    "    print(\"Current Article\" + str(i))\n",
    "    #ent_val = validation_data[validation_data[\"text_id\"]==data[\"id\"][i]]\n",
    "    #rel_val = relation_validation_data[relation_validation_data[\"text_id\"]==data[\"id\"][i]]\n",
    "    article = Article(data[\"text\"][i], data[\"author_extracted\"][i], data[\"date\"][i], data[\"source\"][i], data[\"id\"][i],\n",
    "               download_ner_model=True, n_shift=0,graph=g,first_round=False ,commit=True,\n",
    "               initial_entities = initial_entities_df,initial_syn_dict = initial_entities_dict,\n",
    "                validate=False, filter_by_constituency=True,use_coref = False,\n",
    "                     path_constituency_parser=\"/home/ubuntu/ma_project/elmo-constituency-parser-2020.02.10.tar.gz\",\n",
    "                     path_coref_parser=\"/home/ubuntu/ma_project/coref-spanbert-large-2020.02.27.tar.gz\",\n",
    "                     constituency_predictor = constituency_predictor_1)\n",
    "    article.process_articles()\n",
    "global_stop = time.time()\n",
    "print(\"time for third quater articles: \", global_stop-global_start)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_entities_dict, initial_entities_df = create_synonym_dict(g, None)\n",
    "global_start = time.time()\n",
    "for i in range(10, 15):\n",
    "    print(\"Current Article\" + str(i))\n",
    "    #ent_val = validation_data[validation_data[\"text_id\"]==data[\"id\"][i]]\n",
    "    #rel_val = relation_validation_data[relation_validation_data[\"text_id\"]==data[\"id\"][i]]\n",
    "    article = Article(data[\"text\"][i], data[\"author_extracted\"][i], data[\"date\"][i], data[\"source\"][i], data[\"id\"][i],\n",
    "               download_ner_model=True, n_shift=0,graph=g,first_round=False ,commit=True,\n",
    "               initial_entities = initial_entities_df,initial_syn_dict = initial_entities_dict,\n",
    "                validate=False, filter_by_constituency=True,use_coref = False,\n",
    "                     path_constituency_parser=\"/home/ubuntu/ma_project/elmo-constituency-parser-2020.02.10.tar.gz\",\n",
    "                     path_coref_parser=\"/home/ubuntu/ma_project/coref-spanbert-large-2020.02.27.tar.gz\",\n",
    "                     constituency_predictor = constituency_predictor_1)\n",
    "    article.process_articles()\n",
    "global_stop = time.time()\n",
    "print(\"time for third quater articles: \", global_stop-global_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establishe DB Connection\n",
    "global g\n",
    "g = connect_database(db_name = \"testimprovements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "constituency_predictor_1 = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/elmo-constituency-parser-2020.02.10.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Article10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/CSS_Termpaper/EntityNetwork/entitynetwork/relation_extraction/relation_extraction.py:58: RuntimeWarning: Mean of empty slice\n",
      "  sentiment = np.nanmean(prod)\n",
      "Your label namespace was 'pos'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n",
      "Your label namespace was 'pos'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relation Extraction: 0.1470506191253662\n",
      "constituency took:  6.823678493499756\n",
      "constituency took:  7.568568468093872\n",
      "constituency took:  7.693061351776123\n",
      "constituency took:  8.37977647781372\n",
      "Filter Relations: 8.493813037872314\n",
      "__ArticleNode: 0.0004811286926269531\n",
      "__Publuischer: 0.00011587142944335938\n",
      "__Author: 7.677078247070312e-05\n",
      "__push article_node: 0.007390022277832031\n",
      "__push author_node: 0.0038993358612060547\n",
      "__push publisher_node: 0.0035674571990966797\n",
      "__commit_relationships: 0.2159867286682129\n",
      "__calculate_sub_graph_metrics: 0.3555870056152344\n",
      "Commit to db: 0.5883235931396484\n",
      "Current Article11\n",
      "Relation Extraction: 0.1658329963684082\n",
      "constituency took:  4.221751928329468\n",
      "constituency took:  7.590617656707764\n",
      "constituency took:  8.018812894821167\n",
      "constituency took:  9.373486280441284\n",
      "Filter Relations: 9.42567229270935\n",
      "__ArticleNode: 0.0005161762237548828\n",
      "__Publuischer: 0.00010013580322265625\n",
      "__Author: 0.0007801055908203125\n",
      "__push article_node: 0.010820865631103516\n",
      "__push author_node: 0.008863687515258789\n",
      "__push publisher_node: 0.012710332870483398\n",
      "__commit_relationships: 0.3016948699951172\n",
      "__calculate_sub_graph_metrics: 0.15080928802490234\n",
      "Commit to db: 0.4880845546722412\n",
      "Current Article12\n",
      "Relation Extraction: 0.38664817810058594\n",
      "constituency took:  9.662992000579834\n",
      "constituency took:  18.345837831497192\n",
      "constituency took:  19.03677248954773\n",
      "constituency took:  19.29228377342224\n",
      "Filter Relations: 19.46901035308838\n",
      "__ArticleNode: 0.0006930828094482422\n",
      "__Publuischer: 9.250640869140625e-05\n",
      "__Author: 9.584426879882812e-05\n",
      "__push article_node: 0.008789777755737305\n",
      "__push author_node: 0.005071878433227539\n",
      "__push publisher_node: 0.00491786003112793\n",
      "__commit_relationships: 0.2523512840270996\n",
      "__calculate_sub_graph_metrics: 0.2854306697845459\n",
      "Commit to db: 0.5583374500274658\n",
      "Current Article13\n",
      "Relation Extraction: 0.2514357566833496\n",
      "constituency took:  5.326313257217407\n",
      "constituency took:  7.254512071609497\n",
      "constituency took:  7.386605262756348\n",
      "constituency took:  8.403615951538086\n",
      "Filter Relations: 8.59021520614624\n",
      "__ArticleNode: 0.00039577484130859375\n",
      "__Publuischer: 9.012222290039062e-05\n",
      "__Author: 6.651878356933594e-05\n",
      "__push article_node: 0.005297422409057617\n",
      "__push author_node: 0.0062792301177978516\n",
      "__push publisher_node: 0.005668163299560547\n",
      "__commit_relationships: 0.34248828887939453\n",
      "__calculate_sub_graph_metrics: 0.10830497741699219\n",
      "Commit to db: 0.4700651168823242\n",
      "Current Article14\n",
      "Relation Extraction: 0.16887927055358887\n",
      "constituency took:  6.563683271408081\n",
      "constituency took:  7.27009654045105\n",
      "constituency took:  8.159771919250488\n",
      "constituency took:  9.211852073669434\n",
      "Filter Relations: 9.273101806640625\n",
      "__ArticleNode: 0.0003273487091064453\n",
      "__Publuischer: 0.0009355545043945312\n",
      "__Author: 0.00010228157043457031\n",
      "__push article_node: 0.0057544708251953125\n",
      "__push author_node: 0.0040166378021240234\n",
      "__push publisher_node: 0.003931283950805664\n",
      "__commit_relationships: 0.2596609592437744\n",
      "__calculate_sub_graph_metrics: 0.1293163299560547\n",
      "Commit to db: 0.4050867557525635\n",
      "time for third quater articles:  59.1411919593811\n"
     ]
    }
   ],
   "source": [
    "initial_entities_dict, initial_entities_df = create_synonym_dict(g, None)\n",
    "global_start = time.time()\n",
    "for i in range(10, 15):\n",
    "    print(\"Current Article\" + str(i))\n",
    "    #ent_val = validation_data[validation_data[\"text_id\"]==data[\"id\"][i]]\n",
    "    #rel_val = relation_validation_data[relation_validation_data[\"text_id\"]==data[\"id\"][i]]\n",
    "    article = Article(data[\"text\"][i], data[\"author_extracted\"][i], data[\"date\"][i], data[\"source\"][i], data[\"id\"][i],\n",
    "               download_ner_model=True, n_shift=0,graph=g,first_round=False ,commit=True,\n",
    "               initial_entities = initial_entities_df,initial_syn_dict = initial_entities_dict,\n",
    "                validate=False, filter_by_constituency=True,use_coref = False,\n",
    "                     path_constituency_parser=\"/home/ubuntu/ma_project/elmo-constituency-parser-2020.02.10.tar.gz\",\n",
    "                     path_coref_parser=\"/home/ubuntu/ma_project/coref-spanbert-large-2020.02.27.tar.gz\",\n",
    "                     constituency_predictor = constituency_predictor_1)\n",
    "    article.process_articles()\n",
    "global_stop = time.time()\n",
    "print(\"time for third quater articles: \", global_stop-global_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from entitynetwork.enititynetwork_pipeline.process_article import *\n",
    "from entitynetwork.database_creation.fill_database import *\n",
    "from entitynetwork.relation_extraction.relation_extraction import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "g = connect_database()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.362269639968872\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "test_dict = create_synonym_dict(g, None)\n",
    "stop = time.time()\n",
    "print(stop-start)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "21571"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(test_dict[1][\"id\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "new_df = g.run('MATCH (n:Entity) WHERE ID(n) > 21571 RETURN n.name AS name, n.tag AS tag, n.synonyms AS synonyms, '\n",
    "                           'n.locked AS locked, ID(n) AS id').to_data_frame()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                 name          tag  \\\n0                                       sarah sanders       PERSON   \n1                                 mueller report news  WORK_OF_ART   \n2                                          chuck todd       PERSON   \n3                                    reliable sources  WORK_OF_ART   \n4                                         matt taibbi       PERSON   \n..                                                ...          ...   \n71                                american family act          LAW   \n72                                      susan delbene       PERSON   \n73  politics:inside the white house fight over oba...  WORK_OF_ART   \n74                                      mick mulvaney       PERSON   \n75                                    william p. barr       PERSON   \n\n                                             synonyms locked     id  \n0                                     _sarah sanders_   None  21598  \n1               _mueller report news__mueller report_   None  21617  \n2                                        _chuck todd_   None  21618  \n3                                  _reliable sources_   None  21619  \n4                               _matt taibbi__taibbi_   None  21620  \n..                                                ...    ...    ...  \n71                              _american family act_   None  21905  \n72                                    _susan delbene_   None  21906  \n73  _politics:inside the white house fight over ob...   None  21914  \n74                          _mick mulvaney__mulvaney_   None  21915  \n75                                  _william p. barr_   None  21916  \n\n[76 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>tag</th>\n      <th>synonyms</th>\n      <th>locked</th>\n      <th>id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>sarah sanders</td>\n      <td>PERSON</td>\n      <td>_sarah sanders_</td>\n      <td>None</td>\n      <td>21598</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>mueller report news</td>\n      <td>WORK_OF_ART</td>\n      <td>_mueller report news__mueller report_</td>\n      <td>None</td>\n      <td>21617</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>chuck todd</td>\n      <td>PERSON</td>\n      <td>_chuck todd_</td>\n      <td>None</td>\n      <td>21618</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>reliable sources</td>\n      <td>WORK_OF_ART</td>\n      <td>_reliable sources_</td>\n      <td>None</td>\n      <td>21619</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>matt taibbi</td>\n      <td>PERSON</td>\n      <td>_matt taibbi__taibbi_</td>\n      <td>None</td>\n      <td>21620</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>71</th>\n      <td>american family act</td>\n      <td>LAW</td>\n      <td>_american family act_</td>\n      <td>None</td>\n      <td>21905</td>\n    </tr>\n    <tr>\n      <th>72</th>\n      <td>susan delbene</td>\n      <td>PERSON</td>\n      <td>_susan delbene_</td>\n      <td>None</td>\n      <td>21906</td>\n    </tr>\n    <tr>\n      <th>73</th>\n      <td>politics:inside the white house fight over oba...</td>\n      <td>WORK_OF_ART</td>\n      <td>_politics:inside the white house fight over ob...</td>\n      <td>None</td>\n      <td>21914</td>\n    </tr>\n    <tr>\n      <th>74</th>\n      <td>mick mulvaney</td>\n      <td>PERSON</td>\n      <td>_mick mulvaney__mulvaney_</td>\n      <td>None</td>\n      <td>21915</td>\n    </tr>\n    <tr>\n      <th>75</th>\n      <td>william p. barr</td>\n      <td>PERSON</td>\n      <td>_william p. barr_</td>\n      <td>None</td>\n      <td>21916</td>\n    </tr>\n  </tbody>\n</table>\n<p>76 rows  5 columns</p>\n</div>"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}